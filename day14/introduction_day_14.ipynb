{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to fixed / random effects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need to know :\n",
    "    - what is the variance of a linear combinaison of variables\n",
    "    - play with the kroneker product\n",
    "    - look at scipy.stat module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### First, checkout pna2015 and create your own day14 branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance of linear combinaison of (random) variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's $\\textbf{y}$ for instance the value of a voxel in an fmri image) be our variable. If we repeat the experiment, we will observe $y_1, y_2, ..$. $\\textbf{y}$ could also be the value of a contrast for a given subject, and we observe multiple subjects. \n",
    "\n",
    "\n",
    "Let's $\\textbf{z}$ be another variable. \n",
    "\n",
    "Computing the variance (or, estimating the variance) of $\\textbf{y}$ or $\\textbf{z}$ necessitate that we have several observations.\n",
    "\n",
    "The variance is defined as the true mean of the squared deviation of the observation to the true mean of the data. We need to estimate this. \n",
    "\n",
    "First, we estimate the mean : $\\bar{y} = \\frac{1}{n}\\sum_i^n{y_i}$\n",
    "Second we estimate the variation around this mean: \n",
    "\n",
    "$$ \\widehat{\\textrm{Var}}(\\textbf{y}) = \\frac{1}{n}\\sum_i^n{(y_i - \\bar{y})}^2$$\n",
    "\n",
    "Notice again that this is just an estimation. The true variance is a fixed number, but we will get a different answer each time we have a different dataset. \n",
    "\n",
    "The covariance of two variables can be estimated with: \n",
    "\n",
    "$$ \\widehat{\\textrm{Cov}}(\\textbf{y}, \\textbf{z}) \n",
    "= \\frac{1}{n}\\sum_i^n{(y_i - \\bar{y})(z_i - \\bar{z})}$$\n",
    "\n",
    "\n",
    "Notice that the covariance between $x$ and $x$ is the variance of $x$, and that the covariance is symetric: Cov(x,y) = Cov(y,x). Covariances are closely related to the Pearson correlation coefficient $c$, $c(x,y) = \\frac{\\textrm{Cov}(x,y)}{\\sqrt(\\textrm{Var}(x))\\sqrt(\\textrm{Var}(y))}$\n",
    "\n",
    "Now, we can show that:\n",
    "\n",
    "$$ \\textrm{Cov}(\\textbf{x} + \\textbf{y}, \\textbf{z}) = \\textrm{Cov}(\\textbf{x}, \\textbf{z}) + \\textrm{Cov}(\\textbf{y}, \\textbf{z})\n",
    "$$\n",
    "\n",
    "and that\n",
    "\n",
    "$$ \\textrm{Cov}(a\\textbf{y}, \\textbf{z}) = a\\textrm{Cov}\\textrm{Cov}(\\textbf{y}, \\textbf{z})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$ \\textrm{Var}(\\textbf{y} + \\textbf{z}) = \\textrm{Var}(\\textbf{y}) + \\textrm{Var}(\\textbf{z}) + 2\\textrm{Cov}(\\textbf{y},\\textbf{z})\n",
    "$$\n",
    "\n",
    "and \n",
    "\n",
    "$$ \\textrm{Var}(a \\, \\textbf{y}) = a^2 \\textrm{Var}(\\textbf{y}) $$\n",
    "\n",
    "Above, we droped the estimation sign (the hat) because these relations are true not only for our estimated values as defined above, but for the actual variances and covariances. You can check these relations easily with a given set of values taken by the variables.\n",
    "\n",
    "Now this is the result that will be so useful in countless situations. Imaging that instead of having one variable $x$, we have a set of variables that we can represent by a vector of say, $p$ variables. Let's call this set of variable a vector $\\overrightarrow{\\bf{x}}$, each element of $\\overrightarrow{\\bf{x}}$ is one variable ${\\bf{x_i}}$, and there are $p$ of those. Now, if I do my experiment again and again, say n times, I will observe n time the vector x, therefore n times a vector of p numbers. The variance-covariance of this vector $\\overrightarrow{\\bf{x}}$ is by definition the $p\\times p$ matrix $\\bf{V_x}$ with each element of $\\bf{V_x}$ being:\n",
    "\n",
    "$$ \\bf{V_x(i,j)} = \\textrm{Cov}(\\bf{x_i}, \\bf{x_j})$$\n",
    "\n",
    "\n",
    "So, for instance, if the p variables are not correlated, the (actual) variance covariance matrix will be diagonal, all the off diagonal terms will be 0. The estimated one of course could and most likely will have non zero values off the diagonal. \n",
    "\n",
    "And there is the great thing. Imaging you are taking linear combinaisons of the $\\bf{x_i}$. You therefore are creating new variables $\\bf{y_i}$ that are just sums and scalings of the initial $\\bf{x_i}$.  This can be represented as a multiplication by a matrix, say $\\bf{A}$, with $\\overrightarrow{\\bf{y}} = \\bf{A}\\overrightarrow{\\bf{x}}$. \n",
    "\n",
    "What is the variance covariance of $\\overrightarrow{\\bf{y}}$ ?  It is :\n",
    "\n",
    "$$ \\bf{V_y} = \\bf{A} \\bf{V_x} \\bf{A}^T $$ \n",
    "\n",
    "If you take $p=2$, this is simple to check. Let's check that experiment agree with theory. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a vector of 3 independant random variables 0,2\n",
    "import numpy as np\n",
    "\n",
    "x = np.random.normal(0,2, size=(3,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.09072634]\n",
      " [-0.58351668]\n",
      " [-4.59530207]]\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1 -1  0]\n",
      " [ 0  1 -1]\n",
      " [ 1  1  0]]\n",
      "\n",
      " y = A x : \n",
      "\n",
      " [[-1.50720966]\n",
      " [ 4.01178539]\n",
      " [-2.67424302]] = \n",
      " [[ 1 -1  0]\n",
      " [ 0  1 -1]\n",
      " [ 1  1  0]] [[-2.09072634]\n",
      " [-0.58351668]\n",
      " [-4.59530207]]\n"
     ]
    }
   ],
   "source": [
    "# now, let's mix them and create x1 - x2, x2 - x3, x1 + x2\n",
    "import sympy as smp\n",
    "\n",
    "A = np.asarray([[1, -1, 0], [0, 1, -1], [1, 1, 0]])\n",
    "print(A)\n",
    "\n",
    "#to do : make it lovely with sympy\n",
    "#a = smp.Matrix(A)\n",
    "#b = smp.Matrix(x)\n",
    "#c = smp.Matrix(y)\n",
    "#c = a*b\n",
    "\n",
    "\n",
    "# our new y variables ys are \n",
    "y = A.dot(x)\n",
    "\n",
    "print(\"\\n y = A x : \\n\\n {} = \\n {} {}\".format(y,A,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Estimation of the variance covariance of x: \n",
      "\n",
      "[[  4.01042282e+00   2.94963494e-03   1.27249148e-03]\n",
      " [  2.94963494e-03   3.99703007e+00  -1.05341850e-03]\n",
      " [  1.27249148e-03  -1.05341850e-03   4.00782911e+00]]\n",
      "\n",
      " compare with the theoretical variance covariance of x:\n",
      "[[ 4.  0.  0.]\n",
      " [ 0.  4.  0.]\n",
      " [ 0.  0.  4.]]\n",
      "\n",
      " Close ! we get better results if n increases ...\n",
      "\n",
      "\n",
      " What is the estimated covariance of y ? \n",
      "\n",
      "[[ 8.00155363 -3.99640635  0.01339275]\n",
      " [-3.99640635  8.00696602  3.99976063]\n",
      " [ 0.01339275  3.99976063  8.01335217]]\n",
      "\n",
      " compare with the theoretical variance covariance of y : A V_x A.T:\n",
      "[[ 8. -4.  0.]\n",
      " [-4.  8.  4.]\n",
      " [ 0.  4.  8.]]\n"
     ]
    }
   ],
   "source": [
    "# What is the variance covariance of x ?\n",
    "# first, take many realizations:\n",
    "\n",
    "n = 100000\n",
    "x = np.random.normal(0,2, size=(3,n))\n",
    "\n",
    "print(\"\\n Estimation of the variance covariance of x: \\n\")\n",
    "\n",
    "print(np.cov(x))\n",
    "\n",
    "print(\"\\n compare with the theoretical variance covariance of x:\")\n",
    "\n",
    "V_x = 4*np.eye(3)\n",
    "print(V_x)\n",
    "\n",
    "print(\"\\n Close ! we get better results if n increases ...\\n\")\n",
    "\n",
    "# What is the covariance of y ?\n",
    "print(\"\\n What is the estimated covariance of y ? \\n\")\n",
    "      \n",
    "y = A.dot(x)\n",
    "\n",
    "print(np.cov(y))\n",
    "\n",
    "# what is the theoretical covariance ? \n",
    "print(\"\\n compare with the theoretical variance covariance of y : A V_x A.T:\")\n",
    "\n",
    "print(A.dot(V_x).dot(A.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The kroneker product : useful for creating array with patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_matrix = np.asarray([[2, 3],[4,1]])\n",
    "the_identity = np.eye(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = np.kron(the_identity, one_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [4, 1]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 0.,  1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_identity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.,  3.,  0.,  0.],\n",
       "       [ 4.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  2.,  3.],\n",
       "       [ 0.,  0.,  4.,  1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = np.array([1, -1, 0, 0 ])\n",
    "o = np.ones((3))\n",
    "k = np.kron(o, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1., -1.,  0.,  0.,  1., -1.,  0.,  0.,  1., -1.,  0.,  0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
