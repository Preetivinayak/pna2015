{
 "metadata": {
  "name": "",
  "signature": "sha256:79dfe5b8eb46425ae441425d6f7e8a026aa064b14d4d84aa89dbe294277400c7"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Parameter estimates for correlated regressors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "By JB Poline and Matthew Brett."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compatibility with Python 3\n",
      "from __future__ import print_function  # print('me') instead of print 'me'\n",
      "from __future__ import division  # 1/2 == 0.5, not 0"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Array and plotting libraries\n",
      "import numpy as np\n",
      "import numpy.linalg as npl\n",
      "import matplotlib.pyplot as plt"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Display plots inside the notebook\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make numpy print 4 significant digits for prettiness\n",
      "np.set_printoptions(precision=4, suppress=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Set random number seed to make random numbers reproducible\n",
      "np.random.seed(42)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## The linear model again"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have some vector of data $\\vec{y}$ with values $y_1, y_2, ..., y_N$.\n",
      "\n",
      "We have one or more vectors of regressors $\\vec{x_1}, \\vec{x_2}, ..., \\vec{x_P}$, where $\\vec{x_1}$ has values $x_{1,1}, x_{1,2}, ..., x_{1,N}$, and $\\vec{x_p}$ has values $x_{p,1}, x_{p,2}, ..., x_{p,N}$.\n",
      "\n",
      "Our linear model says that:\n",
      "\n",
      "$$\n",
      "\\vec{y} = \\beta_1 \\vec{x_1} + \\beta_2 \\vec{x_2} +  ... + \\beta_P \\vec{x_P} + \\vec{\\varepsilon}\n",
      "$$\n",
      "\n",
      "Here:\n",
      "\n",
      "* $\\beta_1, \\beta_2, ... \\beta_P$ are scaling coefficients for vectors $\\vec{x_1}, \\vec{x_2}, ..., \\vec{x_P}$ respectively;\n",
      "* $\\vec{\\varepsilon} = \\varepsilon_1, \\varepsilon_2, ... \\varepsilon_N$ are the remaining unexplained errors for each observation.\n",
      "\n",
      "Usually one of vectors $\\vec{x}$ is a vector of constant value 1.  This models the intercept of the regression model.  We will write this special vector as $\\vec{1}$.\n",
      "\n",
      "As we saw in the [introduction to the general linear model](http://perrin.dynevor.org/glm_intro.html), we can express this same linear model as matrices.  We:\n",
      "\n",
      "* assemble the $\\vec{x_p}$ vectors as columns in a design matrix $\\mathbf{X} = [\\vec{x_1}, \\vec{x_2}, ... \\vec{x_P}]$;\n",
      "* assemble the $\\beta_p$ coefficients into a vector $\\vec{\\beta} = \\beta_1, \\beta_2, ..., \\beta_P$.\n",
      "\n",
      "Then matrix multiplication does the rest:\n",
      "\n",
      "$$\n",
      "\\vec{y} = \\mathbf{X} \\cdot \\vec{\\beta} + \\vec{\\varepsilon}\n",
      "$$\n",
      "\n",
      "To solve for this and find an estimate of $\\beta$, we use the OLS with\n",
      "\n",
      "$$\n",
      "\\mathbf{X}^t\\vec{y} = \\mathbf{X}^t\\mathbf{X} \\cdot \\vec{\\beta}\n",
      "\\\\\n",
      "(\\mathbf{X}^t\\mathbf{X})^{-1} \\mathbf{X}^t \\vec{y} = \\hat{\\vec{\\beta}}\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our first exercise will be to implement the linear model and solve for the parameters."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Models with correlated regressors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some correlated regressors"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Imagine we have a TR (image) every 2 seconds, for 30 seconds.  Here are the times of the TR onsets, in seconds:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "times = np.arange(0, 30, 2)\n",
      "times"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we make a hemodynamic response function (HRF) shape for an event starting at time 0.  Call this `hrf1`:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Gamma distribution from scipy\n",
      "from scipy.stats import gamma\n",
      "\n",
      "# Make SPM-like HRF shape with gamma for peak and undershoot\n",
      "hrf1 = gamma.pdf(times, 6) - 0.35 * gamma.pdf(times, 12)\n",
      "# Scale area under curve to 1\n",
      "hrf1 = hrf1 / np.sum(hrf1)\n",
      "# Plot the hrf with title 'HRF at t=0 (not mean-centered)' \n",
      "\n",
      "# ...."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We have our model (hrf1). Let's simulate some data that follow this model by adding some random gaussian noise, and find out the parameter estimate. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generate data Y, the noise e, generate Y given  X = hrf1 with noise N(0,1)\n",
      "noise = np.random.normal(0,1, size=times.shape)\n",
      "b = 2.0\n",
      "X = hrf1\n",
      "Y = X*b + noise\n",
      "# plot this signal\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# now solve for b: find an estimate - careful, we need matrices, not one dimensional vectors\n",
      "X = X.reshape(times.shape[0],1)\n",
      "Y = Y.reshape(times.shape[0],1)\n",
      "#...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Investigate : \n",
      "    # re-run the experiment above to see the noise influence, \n",
      "    # average 10000 estimates of b (bhat)\n",
      "    # display the distribution of these estimates\n",
      "    # what can you do to reduce the variance of the estimates ?\n",
      "    # Further, we predict the variance of this distribution - see further down ..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Add a correlated regressor to the model:"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we make another HRF starting at t=2 (at the beginning of the second TR):"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# HRF starting at t=2\n",
      "hrf2 = np.zeros(hrf1.shape)\n",
      "hrf2[1:] = hrf1[0:-1]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For simplicity, we remove the mean from these regressors.  We do this to make the HRF regressors independent of the mean ($\\vec{1}$) regressor, but it may not be clear yet why that is a good idea."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Remove the mean from both HRF regressors\n",
      "hrf1 = (hrf1 - hrf1.mean())\n",
      "hrf2 = (hrf2 - hrf2.mean())\n",
      "plt.plot(times, hrf1, label='hrf1 start t=0')\n",
      "plt.plot(times, hrf2, label='hrf2 start t=2')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "These `hrf1` and `hrf2` regressors are correlated.  The Pearson correlation coefficient between the HRFs is:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# what is the correlation between these regressors?\n",
      "# ..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Some simulated data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we are going to make some simulated data from the *signal* formed from the correlated regressors, plus some random *noise*.\n",
      "\n",
      "The *signal* comes from the sum of `hrf1` and `hrf2`.  This simulates the occurence of two events, one starting at t=0, one at t=2, both causing an HRF response of the same magnitude  1.:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "signal = hrf1 + hrf2\n",
      "plt.plot(hrf1, label='hrf1')\n",
      "plt.plot(hrf2, label='hrf2')\n",
      "plt.plot(signal, label='signal (combined hrfs)')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The simulated data is this signal combined with some random noise:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noise = np.random.normal(0,1, size=times.shape)\n",
      "Y = signal + noise\n",
      "plt.plot(times, signal, label='signal')\n",
      "plt.plot(times, Y, '+', label='signal + noise')\n",
      "plt.legend()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We are going apply several linear models to these simulated data.\n",
      "\n",
      "All our models include a regressor of a vector of ones, $\\vec{1}$, modeling the mean.\n",
      "\n",
      "We will call our `hrf1` vector $\\vec{h_1}$.  Call `hrf2` : $\\vec{h_2}$.\n",
      "\n",
      "Our models are:\n",
      "\n",
      "* A model with $\\vec{x}$ vectors $\\vec{h_1}, \\vec{1}$ - single HRF model);\n",
      "* A model with $\\vec{h_1}, \\vec{h_2}, \\vec{1}$ - both HRFs model;\n",
      "* A model with $\\vec{h_1}, \\vec{w}, \\vec{1}$, where $\\vec{w}$ is $\\vec{h_2}$ (`hrf2`) *orthogonalized with respect to* $\\vec{h_1}$ (`hrf1`).  We explain what we mean by this further down the page.\n",
      "\n"
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "First, the model with $\\vec{h_1}, \\vec{1}$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Design matrix for single HRF model\n",
      "X_s = np.vstack((hrf1, np.ones_like(hrf1))).T\n",
      "plt.imshow(X_s, interpolation='nearest', cmap='gray')\n",
      "plt.title('Model with hrf1 regressor')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Simulating the effect of noise on parameter estimates"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Create array of simulated data vectors in columns\n",
      "n_times = len(times) # number of elements in single data vector  \n",
      "n_data_vectors = 100000 \n",
      "# Make many noise vectors (new noise for each column)\n",
      "#...\n",
      "# add signal to make data vectors\n",
      "# Use numpy broadcasting to add vector elementwise to 2D array - call this vector Ys\n",
      "#...\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We first fit the model with only the first HRF regressor to every (signal + noise) sample vector.\n",
      "\n",
      "We will fit our model for each data vector, to make a estimated parameter vector for each data vector.  We can stack these estimated parameter vectors into a 2 by `n_data_vectors` array.\n",
      "\n",
      "Call this array $\\beta^s$ (where the $s$ superscript is for the *single* HRF model)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit X_one to signals + noise\n",
      "n_regressors = X_s.shape[1]\n",
      "# beta (parameter estimate) matrix, one column per data vector\n",
      "B_s = np.zeros((n_regressors, n_data_vectors))\n",
      "# Estimate the parameters of the model for each data vector \n",
      "X_pinv = npl.pinv(X_s)\n",
      "\n",
      "# do a loop to find the B_s[]\n",
      "#...\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In fact, because of the way that matrix multiplications works, we can do exactly the same calculation as we did in the loop above, in one matrix multiplication:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# do it with matrix multiplication, call it B_again\n",
      "#...\n",
      "# how do you make sure that B_s and B_again are close to each other?\n",
      "#...\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will use this trick to estimate the parameter matrices for the rest of our models."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us look at the variance of the first parameter estimate (the parameter for the `hrf1` regressor).\n",
      "\n",
      "Plot the distribution we observe:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#...\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can compare the observed variance of the first parameter estimate with that expected from the formula:\n",
      "\n",
      "$$\n",
      "\\mathrm{var}(c^T \\hat\\beta) = \\hat{\\sigma}^2 c^T (X^T X)^+ c\n",
      "$$\n",
      "\n",
      "To select only the first regressor, we use a contrast vector of\n",
      "\n",
      "$$\n",
      "c = \\left[\n",
      "\\begin{array}{\\cvec}\n",
      "1 \\\\\n",
      "0 \\\\\n",
      "\\end{array}\n",
      "\\right]\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our $\\hat{\\sigma}^2$ will be close to 1, because we added noise with variance 1:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Estimate sigma^2 for every data vector\n",
      "predicted = X_s.dot(B_s)\n",
      "residuals = Ys - predicted\n",
      "# Residuals have N-P degrees of freedom\n",
      "N = n_times\n",
      "P = npl.matrix_rank(X_s) # number of independent columns in design\n",
      "\n",
      "# use Jeanette Mumford's slides to estimate the variance sigma:\n",
      "#...\n",
      "#...\n",
      "# What is the actual value of sigma ? since you know it, replace it in the formula"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Because ${\\sigma}^2 = 1$, $\\mathrm{var}(c^T \\hat\\beta) = c^T (X^T X)^+ c$. "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_s = np.array([[1], [0]]) # column vector\n",
      "# compute the variance of the vector of beta estimate:\n",
      "\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that the mean of the parameter estimates for $\\vec{h_1} (`hrf1`), is somewhere above one, even though we only added 1 times the first HRF as the signal:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Observed B[0] mean for single hrf model:', np.mean(B_s[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This is because the single first regresssor has to fit *both* $\\vec{h_1}$ in the signal, and as much as possible of $\\vec{h_2}$ in the signal, because there is nothing else in the model to fit $\\vec{h_2}$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Now let us construct the model with both HRFs as regressors:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Design matrix for both HRFs model\n",
      "X_b = np.vstack((hrf1, hrf2, np.ones_like(hrf1))).T\n",
      "plt.imshow(X_b, interpolation='nearest', cmap='gray')\n",
      "plt.title('Model with hrf1, hrf2 regressors')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will call the resulting 3 by `n_data_vectors` parameter array : $\\beta^b$ (where the $b$ superscript is for *both* HRF regressors).\n",
      "\n",
      "We will use the matrix multiplication trick above to fit all the data vectors at the same time:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit parameters of X_both for data Y = signal + noise. Call results B_b\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "What estimates do we get for the first regressor, when we have both regressors in the model?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(B_b[0], bins=100)\n",
      "print('Observed B[0] mean for two hrf model', np.mean(B_b[0]))\n",
      "print('Observed B[0] variance for two hrf model', np.var(B_b[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Two things have happened now we added the second (correlated) $\\vec{h_2}$ regressor.  First, the mean of the parameter for the $\\vec{h_1}$ regressor has dropped to 1, because $\\beta^b_1 \\vec{h_1}$ is no longer having to model the signal due to $\\vec{h_2}$.   Second, the variability of the estimate has increased.  This is what the bottom half of the t-statistic predicts:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Compute the predicted variance for hrf1 parameter in both HRF model using C_b=[1 0 0] contrast \n",
      "#...\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The estimate of the parameter for $\\vec{h_2}$ has a mean of around 1, like the parameter estimates for $\\vec{h_1}$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.hist(B_b[1], bins=100)\n",
      "print('Observed B[1] mean for two hrf model', np.mean(B_b[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This mean of 1 is what we expect because we have $\\vec{h_1} + \\vec{h_2}$ in the signal.  Not surprisingly, the $\\vec{h_2}$ parameter estimate has a similar variability to that for the $\\vec{h_1}$ parameter estimate:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Observed B[1] variance for two hrf model', np.var(B_b[1]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The observed variance is very similar to the predicted variance:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "C_b_1 = np.array([0, 1, 0])[:, None]  # column vector\n",
      "C_b_1.T.dot(npl.pinv(X_b.T.dot(X_b)).dot(C_b_1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The parameter estimates for $\\vec{h_1}$ and $\\vec{h_2}$ are anti-correlated:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Relationship of estimated parameter of hrf1 and hrf2\n",
      "plt.plot(B_b[0], B_b[1], '.')\n",
      "np.corrcoef(B_b[0], B_b[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Orthogonalizing hrf2 with respect to hrf1"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "$\\vec{h_2}$ is correlated with $\\vec{h_1}$.\n",
      "\n",
      "We can therefore think of $\\vec{h_2}$ as the sum of some scaling of $\\vec{h_1}$ plus an extra part that cannot be explained by $\\vec{h_1}$.\n",
      "\n",
      "$$\n",
      "\\vec{h_2} = p \\,\\vec{h_1} + \\vec{w}\n",
      "$$\n",
      "\n",
      "where $p$ is some scalar, and\n",
      "\n",
      "$$\n",
      "\\vec{w} = \\vec{h_2} - p \\, \\vec{h_1}\n",
      "$$\n",
      "\n",
      "To restate, we can think of $\\vec{h_2}$ as the sum of some scalar amount of $\\vec{h_1}$ plus $\\vec{w}$.\n",
      "\n",
      "We want to chose $p$ such that $\\vec{w} = \\vec{h_2} - p \\, \\vec{h_1}$ is orthogonal to $\\vec{h_1}$.  In this case $\\vec{w}$ is the part of $\\vec{h_2}$ that cannot be explained by $\\vec{h_1}$.\n",
      "\n",
      "If $\\vec{w}$ is orthogonal to $\\vec{h_1}$ then we call $\\vec{w}$ : $\\vec{h_2}$ *orthogonalized with respect to* $\\vec{h_1}$.\n",
      "\n",
      "Following the same logic as for [projection](http://practical-neuroimaging.github.io/day7.html#key-video-on-projecting-vectors), given $\\vec{w} - p \\, \\vec{h_1}$ is orthogonal to $\\vec{h_1}$:\n",
      "\n",
      "$$\n",
      "(\\vec{w} - p \\, \\vec{h_1}) \\cdot \\vec{h_1} = 0 \\implies \\\\\n",
      "\\vec{w} \\cdot \\vec{h_1} - p \\, \\vec{h_1} \\cdot \\vec{h_1} = 0 \\implies \\\\\n",
      "\\frac{\\vec{w} \\cdot \\vec{h_1}}{\\vec{h_1} \\cdot \\vec{h_1}} = p\n",
      "$$\n",
      "\n",
      "Put another way, $p (\\vec{h_1})$ such that $\\vec{w} = \\vec{h_2} - p (\\vec{h_1})$ is orthogonal to $\\vec{h_1}$ - is also the projection of $\\vec{h_2}$ onto $\\vec{h_1}$."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Project hrf2 onto hrf1 : implement it !\n",
      "#...\n",
      "#...\n",
      "# Get \\vec{w} = hrf2 - projection\n",
      "#...\n",
      "# w and hrf1 are now orthogonal : check it !\n",
      "#..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Plot the vector parts\n",
      "plt.plot(times, hrf1, label=r'$\\vec{h_1}$')\n",
      "plt.plot(times, hrf2, label=r'$\\vec{h_2}$')\n",
      "plt.plot(times, projection, label=r'$p (\\vec{h_1})$')\n",
      "plt.plot(times, w, label=r'$\\vec{w}$')\n",
      "plt.legend()\n",
      "# hrf1 part of hrf2, plus unique part, equals original hrf2\n",
      "assert np.allclose(hrf2, projection + w)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "How much of the first regressor did we find in the second regressor? does it ring a bell ?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "p"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us rewrite our original model containing `hrf1, hrf2`:\n",
      "\n",
      "$$\n",
      "\\vec{y} = \\beta^b_1 \\vec{h_1} + \\beta^b_2 \\vec{h_2} + \\beta^b_3 1\n",
      "$$\n",
      "\n",
      "where $\\vec{y}$ is our data vector.\n",
      "\n",
      "Now we know we can also write this as:\n",
      "\n",
      "$$\n",
      "\\vec{y} = \\beta^b_1 \\vec{h_1} + \n",
      "  \\beta^b_2 (p (\\vec{h_1}) + \\vec{w}) + \\beta^b_3 1 \\\\\n",
      "= \\beta^b_1 \\vec{h_1} + \n",
      "  \\beta^b_2 p (\\vec{h_1}) + \\beta^b_2 \\vec{w} + \\beta^b_3 1 \\\\\n",
      "= (\\beta^b_1 + p \\beta^b_2) \\vec{h_1} + \\beta^b_2 \\vec{w} + \\beta^b_3 1\n",
      "$$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So, what will happen if we drop $\\vec{h_2}$ from our model and leave only $\\vec{w}$?\n",
      "\n",
      "We have called the parameters from the model with $\\vec{h_1}, \\vec{h_2}$ : $\\beta^b$.  Call the parameters from the model with $\\vec{h_1}, \\vec{w}$ : $\\beta^w$. \n",
      "\n",
      "We can see that we are going to get the exact same fit to the data with these two models if $\\beta^w_2 = \\beta^b_2$ and $\\beta^w_1 = p \\beta^b_2 + \\beta^b_1$."
     ]
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Let us try this new (orthogonolized) model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X_w = np.vstack((hrf1, w, np.ones_like(hrf1))).T\n",
      "plt.imshow(X_w, interpolation='nearest', cmap='gray')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Fit the model\n",
      "B_w = npl.pinv(X_w).dot(Ys)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let us first look at the distribution of $\\beta^w_1$ `== B_w[0]`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Distribution of parameter for hrf1 in orth model\n",
      "plt.hist(B_w[0], bins=100)\n",
      "print('Observed B[0] mean for two hrf orth model',\n",
      "      np.mean(B_w[0]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Notice that $\\beta^w_1$ now has the same values as for the single HRF model  : $\\beta^s_1$:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert np.allclose(B_s[0, :], B_w[0, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It therefore has the same variance, and the predicted variance matches:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print('Observed B[0] variance for two hrf orth model', np.var(B_w[0]))\n",
      "pred_var = C_b.T.dot(npl.pinv(X_w.T.dot(X_w)).dot(C_b))\n",
      "print('Predicted B[0] variance for two hrf orth model', pred_var)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The fact that the single hrf and orthogonalized model parameters match may make sense when we remember that adding the $\\vec{w}$ regressor to the model cannot change the parameter for the $\\vec{h_1}$ regressor as $\\vec{w}$ is orthogonal to $\\vec{h_1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We predicted above that $\\beta^w_2$ would stay the same as $\\beta^b_2$ from the not-orthogonalized model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert np.allclose(B_b[1, :], B_w[1, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We predicted that $\\beta^w_1$ would become $\\beta^b_1 + p \\beta^b_2$ from the not-orthogonalized model:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "predicted_beta1 = B_b[0, :] + p * B_b[1, :]\n",
      "assert np.allclose(predicted_beta1, B_w[0, :])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Our predictions were correct.  So let us revise what happened:\n",
      "\n",
      "* We estimated our original model with correlated $\\vec{h_1}, \\vec{h_2}$ to get corresponding estimated parameters $\\beta^b_1, \\beta^b_2$;\n",
      "* we orthogonalized $\\vec{h_2}$ with respect to $\\vec{h_1}$ to give $p$ and $\\vec{w}$;\n",
      "* we replaced $\\vec{h_2}$ with $\\vec{w}$ in the model, and re-estimated, giving new parameters $\\beta^w_1$ for $\\vec{h_1}$, $\\beta^w_2$ for $\\vec{w}$;\n",
      "* $\\beta^w_2 = \\beta^b_2$ - the parameter for the new orthogonalized regressor is unchanged from the non-orthogonalized case;\n",
      "* $\\beta^w_1 = \\beta^b_1 + p \\beta^b_2$ - the parameter for the *unchanged* regressor has increased by $\\beta^b_2$ times the amount of $\\vec{h_2}$ present in $\\vec{h_1}$."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here we show some example parameters from the three model fits:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example parameters from the single hrf model\n",
      "B_s[:,:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example parameters from the non-orth two-hrf model\n",
      "B_b[:,:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Example parameters from the orth model\n",
      "B_w[:,:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# The parameter for the hrf1 regressor in the non-orth model\n",
      "# is correlated with the parameter for the hrf1 regressor\n",
      "# in the orth model.\n",
      "plt.plot(B_b[0], B_w[0], '.')\n",
      "plt.title('Orth and non-orth hrf1 parameters correlate')\n",
      "np.corrcoef(B_b[0], B_w[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Relationship of estimated parameters for hrf1 and orthogonalized hrf2\n",
      "# (they should be independent)\n",
      "plt.plot(B_w[0], B_w[1], '+')\n",
      "plt.title('hrf1 and orth hrf2 parameters are independent')\n",
      "np.corrcoef(B_w[0], B_w[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Student statistics "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Remember that the students-t statistic is:\n",
      "\n",
      "$$\n",
      "t = \\frac{c^T \\hat\\beta}{\\sqrt{\\mathrm{var}(c^T \\hat\\beta)}}\n",
      "$$\n",
      "\n",
      "where $c^T$ is a row vector of contrast weights, $\\hat{\\beta}$ is our vector of estimated parameters, and $\\mathrm{var}(c^T \\hat\\beta)$ is the variance of $c^T \\hat\\beta$.\n",
      "\n",
      "On the assumption of zero mean normally distributed independent noise:\n",
      "\n",
      "$$\n",
      "\\mathrm{var}(c^T \\hat\\beta) = \\hat{\\sigma}^2 c^T (X^T X)^+ c\n",
      "$$\n",
      "\n",
      "where $\\hat{\\sigma}^2$ is our estimate of variance in the residuals, and $(X^T X)^+$ is the [pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_pseudoinverse) of $X^T X$.\n",
      "\n",
      "Therefore:\n",
      "\n",
      "$$\n",
      "t = \\frac{c^T \\hat\\beta}{\\sqrt{\\hat{\\sigma}^2 c^T (X^T X)^+ c}}\n",
      "$$\n",
      "\n",
      "We will see that this expection of variance correctly predicts that parameter estimates for correlated regressors will have higher variance for a given level of noise (where the level of noise can be captured by $\\hat{\\sigma}^2$).\n",
      "\n",
      "Put another way, the parameter estimates of correlated regressors are more susceptible to the effect of noise.\n",
      "\n",
      "We can look at the variability of the parameter estimates, by estimating our models on many different simulated data vectors.\n",
      "\n",
      "Each of the data vectors are made of the *signal* (the sum of `hrf1` and `hrf2`) and some *noise*.  We take a new sample of noise for each data vector:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}